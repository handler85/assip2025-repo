import json
import os
import subprocess
from tqdm import tqdm # A library for progress bars, run: pip install tqdm

# --- Configuration: Update these paths ---

# The path to the JSON file generated by your previous script
INPUT_JSON_PATH = './Generated_proof/MiniF2F_Valid/test_output/MiniF2F_test_Lean4_proof.json' # IMPORTANT: Update this path

# The path where the final evaluation results will be saved
OUTPUT_JSON_PATH = './evaluation_results.json'

# The path to the Lean 4 project you created for checking proofs
# This directory should contain a lakefile.lean
LEAN_PROJECT_PATH = './LeanErrorChecker'
LEAN_EXEC_PATH = './LeanErrorChecker'

# --- The Worker Function ---

def evaluate_lean_proof(fl_statement, proof_string, lean_project_path, lean_exec_path, temp_filename="TempProof.lean"):
    """
    Evaluates a single Lean proof string by writing it to a file
    and running the Lean compiler on it.
    """
    # This header is crucial. It provides the basic tactics and definitions.
    # You might need to add more imports if your problems use advanced mathlib areas.
    lean_file_header = "import Mathlib\nimport Mathlib.Tactic\nimport Mathlib.Data.Real.Basic\nimport Aesop\n"
    
    # Handle cases where the model might regenerate the theorem statement
    # We only want the proof tactics, so we look for `:= by`
    if ":= by" in proof_string:
        # We assume the user wants the full theorem and proof
        full_lean_code = lean_file_header + proof_string
    else:
        # If the model only generated tactics, we can't compile it alone.
        # This is a basic handler; a more robust solution would re-add the statement.
        full_lean_code = lean_file_header + fl_statement + "\n" + proof_string


    temp_file_path = os.path.join(lean_project_path, temp_filename)

    try:
        # Write the full Lean code to a temporary file inside the Lean project
        with open(temp_file_path, 'w', encoding='utf-8') as f:
            f.write(full_lean_code)

        # Run the Lean compiler on the file.
        # The command must be run from within the project directory.
        result = subprocess.run(
            ['lean', temp_filename],
            capture_output=True,
            text=True,
            cwd=lean_exec_path, # Execute the command from the project root
            timeout=60  # Set a timeout (e.g., 60 seconds) to prevent infinite loops
        )

        # Lean compiler often prints errors to stdout, so we combine both.
        error_message = result.stdout + result.stderr

        if result.returncode == 0 and "unsolved goals" not in error_message:
            return {"status": "success", "error_message": ""}
        else:
            # If there are unsolved goals, it's a failure even with returncode 0
            if "unsolved goals" in error_message:
                return {"status": "failed", "error_message": "Unsolved goals remaining."}
            return {"status": "failed", "error_message": error_message.strip()}

    except subprocess.TimeoutExpired:
        return {"status": "failed", "error_message": "Timeout: Proof compilation took too long."}
    except Exception as e:
        return {"status": "failed", "error_message": f"An unexpected script error occurred: {e}"}
    finally:
        # Clean up the temporary file after evaluation
        if os.path.exists(temp_file_path):
            os.remove(temp_file_path)

# --- The Manager (Main Script Logic) ---

def main():
    """
    Main function to load generated proofs, evaluate them, and save the results.
    """
    print(f"Loading generated proofs from: {INPUT_JSON_PATH}")
    try:
        with open(INPUT_JSON_PATH, 'r', encoding='utf-8') as f:
            all_problems = json.load(f)
    except FileNotFoundError:
        print(f"FATAL: Input file not found at {INPUT_JSON_PATH}. Please check the path.")
        return
    except json.JSONDecodeError:
        print(f"FATAL: Could not parse the JSON file. It might be corrupted.")
        return

    print(f"Found {len(all_problems)} problems to evaluate.")
    
    # This list will store the detailed results for every single proof attempt
    evaluation_results = []
    
    # Use tqdm for a nice progress bar
    for problem in tqdm(all_problems, desc="Evaluating Problems"):
        problem_name = problem.get("Name")
        generated_proofs = problem.get("Generated_proof", [])
        fl_statement = problem.get("FL_statement")
        if not generated_proofs:
            continue # Skip problems where no proofs were generated

        for i, proof_attempt in enumerate(generated_proofs):
            evaluation = evaluate_lean_proof(fl_statement, proof_attempt, LEAN_PROJECT_PATH, LEAN_EXEC_PATH)
            
            # Create a structured record for this attempt
            result_record = {
                "problem_name": problem_name,
                "attempt_index": i,
                "status": evaluation["status"],
                "error_message": evaluation["error_message"],
                "generated_proof": proof_attempt # Keep the original proof for reference
            }
            evaluation_results.append(result_record)

    print(f"\nEvaluation complete. Saving {len(evaluation_results)} results to: {OUTPUT_JSON_PATH}")
    
    # Save the final structured results to a new JSON file
    with open(OUTPUT_JSON_PATH, 'w', encoding='utf-8') as f:
        json.dump(evaluation_results, f, indent=4)

    print("âœ… Done.")

if __name__ == "__main__":
    # This ensures the main() function is called when you run `python evaluate_proofs.py`
    main()